Serving SmolLM inference requests via a Rust load balancer over 5 Docker containers each hosting a SmolLM model. gRPC used to stream tokens to client in real-time.

Work in Progress
